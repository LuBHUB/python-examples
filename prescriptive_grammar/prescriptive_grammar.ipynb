{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prescriptive grammar\n",
    "\n",
    "Here we will illustrate some basic tools for Natural Language Processing (NLP), using the example of searching a text for violations of two contentious rules of good grammar:\n",
    "\n",
    "* A preposition (such as *for*, *to*, etc.) is not a good thing to end a sentence with.\n",
    "* And you shouldn't begin a sentence with a conjunction (such as *and*, *but*, etc.) \n",
    "\n",
    "In my semi-humble opinion, neither of these rules is really worth observing. Descriptively, they both occur regularly in everyday writing, and practically, neither of them obscures or impoverishes the meaning of a sentence. This seems also to be the majority opinion among grammarians. Nonetheless they crop up in antique style guides or conversation with provincial English teachers from time to time. Here they will just serve us as simple starting examples of linguistic phenomena to search for in a text.\n",
    "\n",
    "If this topic has already raised your hackles, you can read one of the recent rounds of the debate in an article in the [New Yorker](https://www.newyorker.com/culture/cultural-comment/steven-pinkers-bad-grammar) rebutting Steven Pinker's ideas on the subject, and Pinker's rebuttal of the rebuttal in the [Guardian](https://www.theguardian.com/books/booksblog/2015/oct/06/steven-pinker-alleged-rules-of-writing-superstitions).\n",
    "\n",
    "## nltk\n",
    "\n",
    "In order to find instances of the two violations described above, we need to search for specific patterns in text. But the patterns we need here go somewhat beyond what is easily achievable with regular expressions. For example for the first one we need to define a set of conjunctions and search for any one of these either right at the beginning of the text or following a sentence end, which itself can occur in several ways (period, question mark, exclamation mark, etc. plus any of these followed by a quote mark or parenthesis). This is doable and could be an instructive exercise if you want to practice regular expressions, but as usual people with more expertise and more time on their hands have gone before and laid some of the groundwork for us already.\n",
    "\n",
    "The *natural language toolkit* (nltk) is a Python package that provides tools for parsing text into sentences, words, etc. as well as models that can recognize the grammatical role of a word (noun, verb, etc.) with a reasonable degree of accuracy. The package is accompanied by a very comprehensive guidebook that is available both [online](https://www.nltk.org/book/) and [in print](http://shop.oreilly.com/product/9780596516499.do).\n",
    "\n",
    "### Corpora\n",
    "\n",
    "Let's begin by downloading and reviewing an example text. One useful feature of nltk is that it provides a downloader for various corpora of text. These include news stories, novels, and various other sources that are already suitable material for many basic research questions. Here we will use the 'gutenberg' corpus, a collection of texts that are freely available via [Project Gutenberg](https://www.gutenberg.org/). Downloading them via nltk rather than going to the Project Gutenberg website ensures that our work is easily reproducible for others who work with nltk.\n",
    "\n",
    "(And in Germany, possibly the only country in the world that takes intellectual property laws seriously, this brings the added bonus of avoiding the blockade of Project Gutenberg for German IP addresses. At the time of writing, the German courts are involved in a legal dispute with the Project Gutenberg Foundation concerning different schedules for copyright expiry in Germany and the US, and a difference in interpretation of jurisdictions over web content. The [legal details](https://cand.pglaf.org/germany/) are actually quite interesting).\n",
    "\n",
    "The `download()` function accepts the name of the corpus (or other downloadable resource) as its first argument. Called without argument, the function opens a graphical interface for selecting specific resources for download. (Note that for me the print output of this command merely confirms that the package has already been downloaded.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/lt/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have downloaded it, we can import our chosen corpus from the `corpus` submodule. The `fileids()` function lists the files in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as example corpora, nltk provides several tools and models for processing text. To determine the structure of a text, an algorithm needs to apply some background knowledge about the structure of a language. This sometimes requires making use of a fairly complex model. The quantity of background knowledge required is not always trivial, so in order to conserve disk space in installation, nltk does not install all this information by default. If we want to use it, we will need to download it first.\n",
    "\n",
    "The easiest way to download all those tools that are likely to be useful for basic tasks is to download the 'popular' bundle. This includes various models for assigning structure to text. (Note that it also includes the Gutenberg corpus, so is a shortcut to also getting hold of the text we are using.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/lt/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the text of Herman Melville's novel *Moby Dick*.\n",
    "\n",
    "The nltk corpora have already been subjected to a fair bit of preprocessing, and many of them are already annotated with additional linguistic information. However, to illustrate working with unpreprocessed text, we will load just the raw text of the novel. This can be done with the `raw()` function of the corpus object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moby Dick by Herman Melville 1851]\r\n",
      "\r\n",
      "\r\n",
      "ETYMOLOGY.\r\n",
      "\r\n",
      "(Supplied by a Late Consumptive Usher to a Grammar School)\r\n",
      "\r\n",
      "The pale Usher--threadbare in coat, heart, body, and brain; I see him\r\n",
      "now.  He was ever dusting his old lexicons and grammars, with a queer\r\n",
      "handkerchief, mockingly embellished with all the gay flags of all the\r\n",
      "known nations of the world.  He loved to dust his old grammars; it\r\n",
      "somehow mildly reminded him of his mortality.\r\n",
      "\r\n",
      "\"While you take in hand to school others, and to teach them by what\r\n",
      "name a whale-fish is to be called in our tongue leaving out, through\r\n",
      "ignorance, the letter H, which almost alone maketh the signification\r\n",
      "of the word, you deliver that which is not true.\" --HACKLUYT\r\n",
      "\r\n",
      "\"WHALE. ... Sw. and Dan. HVAL.  This animal is named from roundness\r\n",
      "or rolling; for in Dan. HVALT is arched or vaulted.\" --WEBSTER'S\r\n",
      "DICTIONARY\r\n",
      "\r\n",
      "\"WHALE. ... It is more immediately from the Dut. and Ger. WALLEN;\r\n",
      "A.S. WALW-IAN, to roll, to wallow.\" --RICHARDSON'S DICTIONARY\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "md = gutenberg.raw('melville-moby_dick.txt')\n",
    "\n",
    "print(md[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "A first common task in nltk is to 'tokenize' the text, or split it into 'tokens': instances of a basic unit of language, such as syllable, word, sentence, etc. Since our task concerns identifying uses of words at the beginning and end of sentences, we need to tokenize the text into sentences first.\n",
    "\n",
    "nltk provides a few tokenizing functions for splitting a text into tokens. The `sent_tokenize()`function tokenizes a text into sentences, returning a list of sentences.\n",
    "\n",
    "(Note that the tokenizing functions depend on our having downloaded the additional nltk resources for processing text as described above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moby Dick by Herman Melville 1851]\r\n",
      "\r\n",
      "\r\n",
      "ETYMOLOGY.\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(md)\n",
    "\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw above that the text begins not immediately with the well-known opening line but with a title and some preamble concerning the etymology of the word *whale*. This etymological preamble too is part of the full text as Melville wrote it (a little known fact that you may once in a lifetime have a chance to impress your friends with), but for illustration purposes we will cut the text so that it begins with the more well-known opener of the novel proper.\n",
    "\n",
    "We can use the text tokenized as sentences to more easily search for a specific sentence, and cut out those sentences preceding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call me Ishmael.\n",
      "Some years ago--never mind how long\r\n",
      "precisely--having little or no money in my purse, and nothing\r\n",
      "particular to interest me on shore, I thought I would sail about a\r\n",
      "little and see the watery part of the world.\n",
      "It is a way I have of\r\n",
      "driving off the spleen and regulating the circulation.\n",
      "Whenever I\r\n",
      "find myself growing grim about the mouth; whenever it is a damp,\r\n",
      "drizzly November in my soul; whenever I find myself involuntarily\r\n",
      "pausing before coffin warehouses, and bringing up the rear of every\r\n",
      "funeral I meet; and especially whenever my hypos get such an upper\r\n",
      "hand of me, that it requires a strong moral principle to prevent me\r\n",
      "from deliberately stepping into the street, and methodically knocking\r\n",
      "people's hats off--then, I account it high time to get to sea as soon\r\n",
      "as I can.\n",
      "This is my substitute for pistol and ball.\n",
      "With a\r\n",
      "philosophical flourish Cato throws himself upon his sword; I quietly\r\n",
      "take to the ship.\n",
      "There is nothing surprising in this.\n",
      "If they but\r\n",
      "knew it, almost all men in their degree, some time or other, cherish\r\n",
      "very nearly the same feelings towards the ocean with me.\n"
     ]
    }
   ],
   "source": [
    "def find_first_sentence_starting_with(sents, substr):\n",
    "    for i, s in enumerate(sents):\n",
    "        if s.startswith(substr):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "start = find_first_sentence_starting_with(sentences, 'Call me Ishmael')\n",
    "sentences = sentences[start:]\n",
    "\n",
    "for i in range(8):\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want each sentence to be tokenized into words. For this we can apply nltk's word tokenizer to each of our sentences. Since it will be convenient later to retain the raw sentences, we assign the word-tokenized sentences into a new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [nltk.word_tokenize(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging\n",
    "\n",
    "Our task requires also knowing something about the grammatical roles of the words in the sentences. For this, we need to 'tag' the tokens with extra information. The information we require concerns the *parts of speech* (POS) of the words in the text. A part of speech is a grammatical role such as verb, noun, modifier, preposition, etc.\n",
    "\n",
    "nltk's POS tagging functions attach a 'tag' to each word token. A tag is a shortened label that identifies the probable POS of the word. The basic function for assigning POS tags to text is `pos_tag()`, but note that its documentation recommends `pos_tag_sents()` instead if we have text that is first tokenized as sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pos_tag in module nltk.tag:\n",
      "\n",
      "pos_tag(tokens, tagset=None, lang='eng')\n",
      "    Use NLTK's currently recommended part of speech tagger to\n",
      "    tag the given list of tokens.\n",
      "    \n",
      "        >>> from nltk.tag import pos_tag\n",
      "        >>> from nltk.tokenize import word_tokenize\n",
      "        >>> pos_tag(word_tokenize(\"John's big idea isn't all that bad.\"))\n",
      "        [('John', 'NNP'), (\"'s\", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is', 'VBZ'),\n",
      "        (\"n't\", 'RB'), ('all', 'PDT'), ('that', 'DT'), ('bad', 'JJ'), ('.', '.')]\n",
      "        >>> pos_tag(word_tokenize(\"John's big idea isn't all that bad.\"), tagset='universal')\n",
      "        [('John', 'NOUN'), (\"'s\", 'PRT'), ('big', 'ADJ'), ('idea', 'NOUN'), ('is', 'VERB'),\n",
      "        (\"n't\", 'ADV'), ('all', 'DET'), ('that', 'DET'), ('bad', 'ADJ'), ('.', '.')]\n",
      "    \n",
      "    NB. Use `pos_tag_sents()` for efficient tagging of more than one sentence.\n",
      "    \n",
      "    :param tokens: Sequence of tokens to be tagged\n",
      "    :type tokens: list(str)\n",
      "    :param tagset: the tagset to be used, e.g. universal, wsj, brown\n",
      "    :type tagset: str\n",
      "    :param lang: the ISO 639 code of the language, e.g. 'eng' for English, 'rus' for Russian\n",
      "    :type lang: str\n",
      "    :return: The tagged tokens\n",
      "    :rtype: list(tuple(str, str))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need to specify what tags we wish to assign to our text. The `tagset` argument to `pos_tag()` allows us to choose a standardized set of POS tags. The simplest of these is the so-called 'universal' tagset [proposed by researchers at Google](https://arxiv.org/pdf/1104.2086.pdf). This is a very simplified set of twelve broad types of words (or parts of words, morphemes etc.) that occur across various languages. You can read a list of the tags and their meanings at the [GitHub page](https://github.com/slavpetrov/universal-pos-tags/blob/master/README) for the universal tagset project.\n",
    "\n",
    "(The functions for assigning parts of speech to words also require the additional nltk data we downloaded above.)\n",
    "\n",
    "Applying a POS model is a fairly computationally intensive operation, so for a long text it may take a moment to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Call', 'VERB'), ('me', 'PRON'), ('Ishmael', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.pos_tag_sents(tokens, tagset='universal')\n",
    "\n",
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word in the sentence is now a tuple of the form *(word, POS)*.\n",
    "\n",
    "### Working with Parts of Speech\n",
    "\n",
    "We can now proceed to our first task: finding prepositions at the end of sentences. For this, we need to process each sentence in two steps:\n",
    "\n",
    "* If the sentence ends with a token tagged as punctuation, ignore this and treat the previous token as the final token in the sentence.\n",
    "* Check the POS tag for the final token to see whether it is a preposition.\n",
    "\n",
    "So we can define a function for applying these two steps. Before charging in on the whole text, it is worth checking a couple of test sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def endswith_preposition(sentence):\n",
    "    if sentence[-1][1] == '.':\n",
    "        endPos = -2\n",
    "    else:\n",
    "        endPos = -1\n",
    "    return sentence[endPos][1] == 'ADP'\n",
    "\n",
    "testNegative = 'This is the type of arrant pedantry up with which I will not put.'\n",
    "testPositive = 'This is the type of arrant pedantry that I will not put up with.'\n",
    "\n",
    "for test in [testNegative, testPositive]:\n",
    "    test = nltk.pos_tag(nltk.word_tokenize(test), tagset='universal')\n",
    "    print(endswith_preposition(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the whole tokenized and tagged text. We use our function this time to get the indices of the sentences that violate the prescription. We can then use these indices with the original list of sentences to get them in their original form without all the tokens and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They must get just as nigh\r\n",
      "the water as they possibly can without falling in.\n",
      "Tell me that.\n",
      "Again, I always go to sea as a sailor, because they make a point of\r\n",
      "paying me for my trouble, whereas they never pay passengers a single\r\n",
      "penny that I ever heard of.\n",
      "Not ignoring what is good, I am\r\n",
      "quick to perceive a horror, and could still be social with it--would\r\n",
      "they let me--since it is but well to be on friendly terms with all\r\n",
      "the inmates of the place one lodges in.\n",
      "Too expensive and\r\n",
      "jolly, again thought I, pausing one moment to watch the broad glare\r\n",
      "in the street, and hear the sounds of the tinkling glasses within.\n",
      "Supper over, the company went back to the bar-room, when, knowing not\r\n",
      "what else to do with myself, I resolved to spend the rest of the\r\n",
      "evening as a looker on.\n",
      "Presently a rioting noise was heard without.\n",
      "I began to twitch all\r\n",
      "over.\n",
      "I then placed the\r\n",
      "first bench lengthwise along the only clear space against the wall,\r\n",
      "leaving a little interval between, for my back to settle down in.\n",
      "There's\r\n",
      "plenty of room for two to kick about in that bed; it's an almighty\r\n",
      "big bed that.\n"
     ]
    }
   ],
   "source": [
    "violations = [i for i, s in enumerate(tokens) if endswith_preposition(s)]\n",
    "\n",
    "for i in range(10):\n",
    "    print(sentences[violations[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the search has worked out more or less as desired, but the results are not perfect. Among the first few hits we already notice that the word *that* has been erroneously tagged as a preposition where it is really being used as an article or pronoun instead. There are also some prepositions that are not really bare prepositions but part of a phrasal verb such as *falling in*.\n",
    "\n",
    "Identifying the grammatical structure of natural language is a hard task of the sort that humans can still do much more accurately than computers.\n",
    "\n",
    "Let's see how we fare with the second task: identifying conjunctions at the beginning of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def startswith_conjunction(sentence):\n",
    "    return sentence[0][1] == 'CONJ'\n",
    "\n",
    "testNegative = 'This is an example of a sentence that does not begin with a conjunction.'\n",
    "testNegative = 'And this is an example of one that does.'\n",
    "\n",
    "for test in [testNegative, testPositive]:\n",
    "    test = nltk.pos_tag(nltk.word_tokenize(test), tagset='universal')\n",
    "    print(startswith_conjunction(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But these are all landsmen; of week days pent up in\r\n",
      "lath and plaster--tied to counters, nailed to benches, clinched to\r\n",
      "desks.\n",
      "But look!\n",
      "And there they\r\n",
      "stand--miles of them--leagues.\n",
      "But here is an artist.\n",
      "But though the picture\r\n",
      "lies thus tranced, and though this pine-tree shakes down its sighs\r\n",
      "like leaves upon this shepherd's head, yet all were vain, unless the\r\n",
      "shepherd's eye were fixed upon the magic stream before him.\n",
      "And still deeper the meaning of\r\n",
      "that story of Narcissus, who because he could not grasp the\r\n",
      "tormenting, mild image he saw in the fountain, plunged into it and\r\n",
      "was drowned.\n",
      "But that same image, we ourselves see in all rivers and\r\n",
      "oceans.\n",
      "And as for going as cook,--though I confess\r\n",
      "there is considerable glory in that, a cook being a sort of officer\r\n",
      "on ship-board--yet, somehow, I never fancied broiling fowls;--though\r\n",
      "once broiled, judiciously buttered, and judgmatically salted and\r\n",
      "peppered, there is no one who will speak more respectfully, not to\r\n",
      "say reverentially, of a broiled fowl than I will.\n",
      "And at first, this sort of\r\n",
      "thing is unpleasant enough.\n",
      "And more than\r\n",
      "all, if just previous to putting your hand into the tar-pot, you have\r\n",
      "been lording it as a country schoolmaster, making the tallest boys\r\n",
      "stand in awe of you.\n"
     ]
    }
   ],
   "source": [
    "violations = [i for i, s in enumerate(tokens) if startswith_conjunction(s)]\n",
    "\n",
    "for i in range(10):\n",
    "    print(sentences[violations[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fare rather better on this task. Probably because there are a lot fewer words that can serve as conjunctions and because these words only very rarely play other grammatical roles.\n",
    "\n",
    "## spacy\n",
    "\n",
    "nltk began life as a teaching tool in linguistics rather than as state-of-the-art NLP software, and this remains its primary application. The tokenizing and tagging tools provided in nltk are in principle extensible with custom-made tokenizing and tagging models, but for even moderately complex research tasks basic nltk is inadequate. If you find that nltk fails to identify the linguistic phenomenon you are interested in, there are other tools available that lie further out along the performance dimension of the simplicity-performance trade-off.\n",
    "\n",
    "For Python, foremost among these tools is currently [spacy](https://spacy.io/). spacy provides more complex language models that can assign POS tags in a way that depends a lot more on the context in which words are used, and can be more easily extended with machine learning techniques. spacy has been fairly extensively optimized so as to make efficient use of memory when processing large texts. And it supports many more languages than just English.\n",
    "\n",
    "There is also a spacy [conference and workshop](https://irl.spacy.io) that this year (2019) takes place here in Berlin.\n",
    "\n",
    "Like nltk, spacy makes use of additional knowledge in the form of fairly large data files that need to be downloaded in addition to the base package. spacy organizes this additional data by language. To download the data for English, you will need to first run spacy as a script using Python from the command line with the following parameters:\n",
    "\n",
    "`python3 -m spacy download en`\n",
    "\n",
    "* `python3` ensures that Python 3 is used. If you prefer to use your system's default Python version or Python 2, replace this with just `python` or `python2`, respectively.\n",
    "* The `-m` option to Python runs a Python module as a script.\n",
    "* The remaining arguments `download` and `en` are passed on to the main spacy file when run as a script, instructing it to download the English language models for tokenizing, tagging, etc.\n",
    "\n",
    "If you have run the above command then you can load spacy's English language model into a spacy NLP object using the `load()`function. Since spacy does some fairly memory-intensive processing, it avoids running out of memory by setting a safe limit on the number of characters that should be processed (by default one million).\n",
    "\n",
    "If we know that the processing that we want to do will be fairly simple and is unlikely to exceed memory requirements, we can increase the maximum allowed length to that of our document (which in this case is not very far over a million characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1242990\n"
     ]
    }
   ],
   "source": [
    "print(len(md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', max_length=len(md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting NLP object is directly callable (i.e. can be used as a function), with a text as its input argument.\n",
    "\n",
    "The way that spacy NLP objects interact with text is organized as a 'pipeline': a sequence of processing steps, each of which takes as its input the output of the previous step. By default the pipeline includes the following steps:\n",
    "\n",
    "* *tokenize*: split the text into tokens, most commonly words\n",
    "* *tag*: tag each token according to its role\n",
    "* *parse*: arrange tokens in a structure of 'dependencies' indicating which other word each word refers to\n",
    "* *name entities*: recognize 'named entities' in the text (for example proper names of countries, people, companies, etc.)\n",
    "\n",
    "The first two of these we have already encountered in nltk. The third is new and provides us with a lot more information about the structure of a text, since in addition to just labelling words with their roles it links words that refer to each other. The fourth is also new, and particularly important for political or commercial applications. You can read more about the default spacy pipeline [here](https://spacy.io/usage/processing-pipelines).\n",
    "\n",
    "We can view the current pipeline of an NLP object via an attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f903635c5f8>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f90305da588>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f90305da5e8>)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't always need every step in the pipeline, and we can save processing time and memory usage by excluding the steps we don't need with the `remove_pipe()` method. We do not need dependency parsing or tagging of named entities.\n",
    "\n",
    "We can also add components. Since our task concerns sentences, we will need a component that parses the text into sentences. spacy already provides a `'sentencizer'` component for this. To add components, we must first initialize them with `create_pipe()`, then add them to our NLP object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f90306d0dd8>),\n",
       " ('tagger', <spacy.pipeline.pipes.Tagger at 0x7f903635c5f8>)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for component in ['parser', 'ner']:\n",
    "    nlp.remove_pipe(component)\n",
    "\n",
    "sentencizer = nlp.create_pipe('sentencizer')\n",
    "nlp.add_pipe(sentencizer, before='tagger')\n",
    "\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the pipeline to process the text of Moby Dick that we loaded above. (This is again computationally intensive and may take a moment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting document object is an iterable of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call\n"
     ]
    }
   ],
   "source": [
    "firstWord = doc[5100]\n",
    "\n",
    "print(firstWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token is a spacy Token object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(firstWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These Token objects have various attributes depending on the processing pipeline that we applied. If the pipeline included POS tagging, then the tokens have `.pos` attributes.\n",
    "\n",
    "One of the ways in which spacy conserves memory when repeatedly processing tokens is by referring to attributes via integer codes (since integers are cheaper to store and quicker to look up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(firstWord.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we will more often want to know the meaning of the POS tags. The human-readable labels for spacy attributes are stored in the same-named attributes followed by an underscore `_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(firstWord.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first word of the novel (*call*) has been tagged as a verb.\n",
    "\n",
    "spacy got this one right. Note that the word *call* can be either a noun or a verb depending on context. It is a noun in such well-known phrases as \"Final call for passenger Tudge on Ryanair flight FR1144\", but a verb in the imperative mood in such phrases as \"[call me](https://www.youtube.com/watch?v=StKVS0eI85I)\" (or if you are younger and a little more timid \"[call me *maybe*](https://www.youtube.com/watch?v=fWNaR-rxAic)\"). Though this distinction may seem trivial, drawing it is an example of a task that is [easy for humans but hard for computers](https://en.wikipedia.org/wiki/Moravec%27s_paradox).\n",
    "\n",
    "Many of spacy's POS tags have intuitive names or names in common with other standard tagsets. For a full listing, see the [documentation here](https://spacy.io/api/annotation#pos-tagging).\n",
    "\n",
    "Because we included a sentencizer in the pipeline, the document has a `.sents` attribute that can be iterated. We can now use this to get the sentences that match our pattern. It is necessary also to update our search functions to take into account spacy's POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def endswith_preposition(sentence):\n",
    "    if sentence[-1].pos_ == 'PUNCT':\n",
    "        endPos = -2\n",
    "    else:\n",
    "        endPos = -1\n",
    "    return sentence[endPos].pos_ == 'ADP'\n",
    "\n",
    "def startswith_conjunction(sentence):\n",
    "    return sentence[0].pos_ == 'CONJ'\n",
    "\n",
    "violations = [s.text for s in doc.sents if endswith_preposition(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy seems to have done somewhat better here at recognizing prepositions only when they are used as prepositions proper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\"No, Sir, 'tis a Right Whale,\" answered Tom; \"I saw his sprout; he\r\n",
      "threw up a pair of as pretty rainbows as a Christian would wish to\r\n",
      "look at.\n",
      "\r\n",
      "\r\n",
      "Again, I always go to sea as a sailor, because they make a point of\r\n",
      "paying me for my trouble, whereas they never pay passengers a single\r\n",
      "penny that I ever heard of.\n",
      "\r\n",
      "\r\n",
      "Presently a rioting noise was heard without.\n",
      " But I lay perfectly still, and resolved not to say a\r\n",
      "word till spoken to.\n",
      " And the man that has anything bountifully\r\n",
      "laughable about him, be sure there is more in that man than you\r\n",
      "perhaps think for.\n",
      "\r\n",
      "\r\n",
      "The bar-room was now full of the boarders who had been dropping in\r\n",
      "the night previous, and whom I had not as yet had a good look at.\n",
      " He charges him thrice the\r\n",
      "usual sum; and it's assented to.\n",
      " Hearing him foolishly fumbling\r\n",
      "there, the Captain laughs lowly to himself, and mutters something\r\n",
      "about the doors of convicts' cells being never allowed to be locked\r\n",
      "within.\n",
      "\r\n",
      "Upon this, I told him that whaling was my own design, and informed\r\n",
      "him of my intention to sail out of Nantucket, as being the most\r\n",
      "promising port for an adventurous whaleman to embark from.\n",
      " The grinning landlord, as well as the\r\n",
      "boarders, seemed amazingly tickled at the sudden friendship which had\r\n",
      "sprung up between me and Queequeg--especially as Peter Coffin's cock\r\n",
      "and bull stories about him had previously so much alarmed me\r\n",
      "concerning the very person whom I now companied with.\n"
     ]
    }
   ],
   "source": [
    "for violation in violations[:10]:\n",
    "    print(violation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
